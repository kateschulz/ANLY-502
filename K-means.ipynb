{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark and SQL\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Mini-Project\").getOrCreate()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer, StopWordsRemover\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csvs\n",
    "subreddit = sc.textFile(\"subreddit_body.csv\")\n",
    "top_1000 = sc.textFile(\"subreddit_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cast indices as strings\n",
    "subreddit.subreddit = subreddit.subreddit.astype(str)\n",
    "top_1000.subreddit = top_1000.subreddit.astype(str)\n",
    "\n",
    "#Left Join datasets on subreddit to only keep top 1,000 subreddits with field for body\n",
    "join_data = top_1000.set_index('subreddit').join(subreddit.set_index('subreddit'))\n",
    "join_data.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where body was deleted or removed\n",
    "join_data = join_data[join_data.body != \"[deleted]\"]\n",
    "join_data = join_data[join_data.body != \"[removed]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create unique list of subreddits\n",
    "distinct_subreddit = join_data.subreddit.unique()\n",
    "\n",
    "#Create a dataframe dictionary for unique subreddits\n",
    "subreddit_dict = {elem : pd.DataFrame for elem in distinct_subreddit}\n",
    "for key in subreddit_dict.keys():\n",
    "    subreddit_dict[key] = join_data[:][join_data.subreddit == key]\n",
    "\n",
    "#Create dataframe for unique subreddits with longest body (most words)\n",
    "max_body = []\n",
    "for key in subreddit_dict.keys():\n",
    "    max_body.append({'Subreddit': key,\n",
    "            'Max_body': np.max(subreddit_dict[key]['body'])})\n",
    "subreddit_maxbody = pd.DataFrame(max_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to spark dataframe\n",
    "df_sub_maxbod = spark.createDataFrame(subreddit_maxbody)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run k-means clustering on body for 10 clusters\n",
    "df_sub_maxbod.cache().count()\n",
    "\n",
    "# tokenize, remove stopwords, and vectorize text\n",
    "tokenizer = Tokenizer(inputCol= \"Max_body\", outputCol=\"tokens\")\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"stopWordsRemovedTokens\")\n",
    "hashingTF = HashingTF(inputCol=\"stopWordsRemovedTokens\", outputCol=\"rawFeatures\", \n",
    "                      numFeatures=3)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "kmeans = KMeans(k=10)\n",
    "\n",
    "# pipeline and fit model\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, kmeans])\n",
    "model = pipeline.fit(df_sub_maxbod)\n",
    "\n",
    "#store results\n",
    "results = model.transform(df_sub_maxbod)\n",
    "results.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of subreddits in each cluster\n",
    "results.groupBy(\"prediction\").count().orderBy(\"prediction\", ascending= True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cluster centers\n",
    "model_stage = model.stages[-1]\n",
    "centers = model_stage.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cluster centers and prediction labels to plot \n",
    "columns = [\"x\", \"y\", \"z\"]\n",
    "pred = range(0,10)  \n",
    "data = centers\n",
    "for_plot = pd.DataFrame(data=data, columns= columns)\n",
    "for_plot.loc[:,'pred'] = pd.Series(pred, index=for_plot.index)\n",
    "\n",
    "#Make color palette\n",
    "colors = ( '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', \n",
    "          '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1')\n",
    "c_list = []\n",
    "for c in colors:\n",
    "    c_list.append([c]*1)\n",
    "\n",
    "#Create plot\n",
    "fig = plt.figure(figsize=(12, 12), dpi=100)\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_title('Plot of Cluster Centers for Each Group')\n",
    "\n",
    "for x,y,z, color, group in zip(for_plot['x'],for_plot['y'],for_plot['z'], c_list, for_plot['pred']):\n",
    "    ax.scatter(x,y,z, c= color, edgecolors='none', s=200, label= group, )\n",
    "    ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label 5 cluster Subreddit\n",
    "pred_5 = results.filter(results.prediction.isin([5]))\n",
    "pred_5.select(\"prediction\", \"Subreddit\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average score of this subreddit\n",
    "sub_df = top_1000.loc[top_1000[\"subreddit\"] == \"anriokita\"]\n",
    "print(\"Average Score:\", sub_df.iloc[0]['avg_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 highest scoring subreddits' clusters\n",
    "pred_top_10 = results.filter(results.Subreddit.isin(list(top_10[\"subreddit\"])))\n",
    "pred_top_10.select(\"prediction\", \"Subreddit\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
