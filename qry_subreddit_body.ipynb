{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark and SQL\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Mini-Project\").getOrCreate()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer, StopWordsRemover\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data files\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", \"AKIAQYYJWECSIGREA3FN\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", \"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\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3.awsAccessKeyId\", \"AKIAQYYJWECSIGREA3FN\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3.awsSecretAccessKey\", \"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\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", \"AKIAQYYJWECSIGREA3FN\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load plain text data\n",
    "data_file = \"s3://mv559/reddit/plain-text\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema\n",
    "fields = [StructField(\"archived\", BooleanType(), True), \n",
    "          StructField(\"author\", StringType(), True),\n",
    "          StructField(\"author_flair_css_class\", StringType(), True),\n",
    "          StructField(\"body\", StringType(), True),\n",
    "          StructField(\"controversiality\", LongType(), True),\n",
    "          StructField(\"created_utc\", StringType(), True),\n",
    "          StructField(\"distinguished\", StringType(), True),\n",
    "          StructField(\"downs\", LongType(), True),\n",
    "          StructField(\"edited\", StringType(), True),\n",
    "          StructField(\"gilded\", LongType(), True), #awarded reddit coins\n",
    "          StructField(\"id\", StringType(), True),\n",
    "          StructField(\"link_id\", StringType(), True),\n",
    "          StructField(\"name\", StringType(), True),\n",
    "          StructField(\"parent_id\", StringType(), True),\n",
    "          StructField(\"retrieved_on\", LongType(), True),\n",
    "          StructField(\"score\", LongType(), True),\n",
    "          StructField(\"score_hidden\", BooleanType(), True),\n",
    "          StructField(\"subreddit\", StringType(), True),\n",
    "          StructField(\"subreddit_id\", StringType(), True),\n",
    "          StructField(\"ups\", LongType(), True)]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe\n",
    "df = sqlContext.read.json(raw_data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up new table and cache \n",
    "df.registerTempTable(\"df_table\")\n",
    "output = sqlContext.sql(\"SELECT * From df_table\")\n",
    "output.registerTempTable('df_cache')\n",
    "sqlContext.cacheTable('df_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query distinct subreddits and first comment\n",
    "subreddit_body = sqlContext.sql(\"\"\"\n",
    "SELECT subreddit, FIRST(body) as body\n",
    "FROM df_cache\n",
    "GROUP BY subreddit\n",
    "\"\"\")\n",
    "subreddit = subreddit_body.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save query output to csv\n",
    "subreddit.to_csv('subreddit_bodies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
